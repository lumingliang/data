1. 数据丢失：同时修改一行数据，覆盖、丢失，可用乐观锁，读写锁（新建一个表，利用insert的写锁来实现获取唯一）
2. 脏读，事务1修改l数据未提交，事务2读取了这个未提交的脏数据
3. 不可重复读，事务1读取的数据，过段时间再读，发现不一样了，原因是被事务2改了删了
4. 幻读，事务1 按相同的条件检索数据，发现两次不一样了，多了，被其他事务插入了

可重读，4
已提交读，事务1 提交了，其他事务就可以读了，3,4


先写入redo log 再提交事务，所以达到了容灾
undo log是指事务开始之前，在操作任何数据之前,首先将需操作的数据备份到一个地方 (Undo Log)。

kafka
分段：减少单个索引文件大小，便于读入内存
分区：利于负载均衡
分区、分段多容易造成随机读写
topic对应多个分段

如通过 TCP 协议请求-应答(request-reply)，发布-订阅(pub-sub)和推送-拖拉(push-pull)，PGM(多点广播)，多种模型

rabbitmq，amqp模型， 路由 ：交换机（类型：直接、扇形（广播）、topic（灵活匹配感兴趣的）、header）
消息回复机制：可以设置自动回复、可以设置应用回复。
通道：将连接共享

kafka 
kafka遵从一般的MQ结构，producer，broker，consumer，以consumer为中心，消息的消费信息保存的客户端consumer上，consumer根据消费的点，从broker上批量pull数据；无消息确认机制。
水平扩展、（再broker上创建topic，然后producer直接发送到该broker上，由zookeper管理这些元信息）

消息重发：）指定时间，log.retention.hours=168；2）指定大小，log.segment.bytes=1073741824。此时就可以通过重置某个topic的offset来是消息重新发送，进行消费

不丢消息：producer有个ack参数，有三个值，分别代表：不在乎是否写入成功、写入leader成功、写入leader和所有reclpica成功；要求非常可靠的话可以牺牲性能设置成最后一种。

mysql 分页，有可能不走索引，优化方案是，写出语句让它走，例如建立复合索引（有where时），或者先存入索引表，然后取id，即es）


稀疏索引：不是针对所有key建立索引，找到一个索引后，还需要直接搜索
查找ID=22222的记录
首先，找到小于等于ID=22222的最大索引条目：ID=10101的索引条目；
然后，从ID=10101的索引条目中记录的指针指向的记录ID=10101开始沿着文件里的指针遍历，直到找到ID=22222的记录；

kafka通过分区来实现同个topic分片，从而提高了吞吐量
　上面说到数据会写入到不同的分区，那kafka为什么要做分区呢？相信大家应该也能猜到，分区的主要目的是：
　　1、 方便扩展。因为一个topic可以有多个partition，所以我们可以通过扩展机器去轻松的应对日益增长的数据量。
　　2、 提高并发。以partition为读写单位，可以多个消费者同时消费数据，提高了消息的处理效率。


redis两种缓存模式

rdb和aof
rdb 为快照，可以设定 save 60 1000 即每60s 有1000个key则缓存一次，速度快、启动快、数据一致性低，fork子进程来实现
aof， 追加形式，可以设置每1s追加，两个都打开生产
aof，先以文本协议写入缓冲文件，缓冲文件同步到aof文件（根据配置是否直接调用系统fsync），aof重写，为了合并一些命令为一条，压缩空间
重写期间，执行的命令会分别发给缓冲区和重写缓冲区，重写完毕后，重写文件覆盖原来的aof文件。
AOF 文件重写并不需要对现有的 AOF 文件进行任何读取、分析或者写入操作，而是通过读取服务器当前的数据库状态来实现的。首先从数据库中读取键现在的值，然后用一条命令去记录键值对，代替之前记录这个键值对的多条命令


一致性哈希算法
预防通过取模方式进行hash负载均衡，当新增或者减少节点时，数据需要大批量的迁移
通过一致性哈希算法：hash后得0-2^32次方某一个值，个节点，组成一个圆环，然后对机器ip等计算哈希， 让节点落在环上某一个位置，然后key也是这样计算，从而寻找顺时针找到的第一台主机即可。

redis slot ： 每个key通过CRC16校验后对16384取模来决定放置哪个槽
redis默认平均分配节点到0-16384个slot


BSON有三个特点：轻量性、可遍历性、高效性。二进制json
BSON相对JSON来说也并没有空间上的优势，比如对{“field”:7}，在JSON的存储上7只使用了一个字节，而如果用BSON，那就是至少4个字节（32位）




hash 算法，将key通过算法映射为数组下标，实现O（1），其中方法是将key拆解为ascii数字，然后相加或者、取幂后加。数字过多需要压缩，即根据需要的空间取余，冲突方法：开放地址（申请多一点的空间）、开链发（使用链表来存）

string 采用一个保存 有string长度信息、剩余空间 的结构体，便于重新分配，防止溢出；list采用ziplist（不保存前后指针，而是存储上一个的长度，通过长度来计算，以时间换空间，当元素超过设定后，采用双向链表
）。跳跃表：采用多层结构保存下标作为索引，提高查询速度，用于有序集合。整数集合（底层采用数组来实现）


栈和堆：栈自动分配，堆是程序员自己申请，栈是连续的，堆不连续。

php7 :减少内存分配次数，多使用栈内存，缓存数组hash值，字符串解析成参数改为宏展开，使用大块连续内存代替小块碎片内存等等
PHP7 为什么这么快？
全新的zval 更节约的空间，栈上分配内存
zend_string 存储字符串的Hash值,数组查询的时候不需要进行Hash计算
在HashTable桶内直接存数据,减少了内存的申请次数,提升了cache命中率和内存访问速度
zend_parse_parameters改为了宏实现,性能提升5%
增加opcode指令 call_user_function,is_init/string/array,strlen,defined函数变成opcode指令,速度更快
排序算法的改进

php 数据结构
typedef struct _zval_struct {
 zvalue_value value;
 zend_uint refcount;
 zend_uchar type;
 zend_uchar is_ref;
} zval;
typedef union _zvalue_value {
 long lval;
 double dval;
 struct {
  char *val;
  int len;
 } str;
 HashTable *ht;
 zend_object_value obj;
} zvalue_value;



cap；c一致性 a 可用性 p 分区容忍性
分布式事务
2pc：先记录日志，协调器发送提交信号。mysql支持
tcc：try检查资源，confirm执行事务（在业务层实现事务），cancel，执行取消操作（失败时候调用该接口）。注意接口幂等性（补偿性，即先预留资源，然后confirm为执行补偿实际性提交）

保证幂等性：缓存结果、状态字段
消息队列：（执行失败会重试）。

io模型：
同步：马上得到结果（要么等待要么返回错误）
异步：通过回调通知结果

阻塞：执行io时候，进程是否一直等待。非阻塞：轮询
多路io复用模型：不断轮询
异步io


快速排序法
取第一个元素作为关键值
尾部找一个比他小的，替换i，首部找一个比他大的，替换j处的空缺，不断替换空缺，碰到指针一致，则在该处填入关键值


微服务：
注册中心：
Spring Cloud Eureka：服务发现，注册服务信息、心跳交互
Netflix Zuul：api网关，类似nginx，将代理api请求

集合：arraylist 数组（自动扩容），linklist（链表来实现），hashmap（采用hash原理，key映射为数组，采用链表或者红黑树来存冲突）、treemap（红黑树）、
hashset，value为一样的，然后实际只保存key。
ConcurrentHashMap ：分段锁，两次hash，基于cas，比较交换（设定值的时候，比较是否还是原来的值（乐观锁））cas是操作系统级别的指令（原子，是几条指令的集合必须一次执行）
hashmap 原因：多线程并发会形成环形的链表，导致死循环
基于cas 和synchronized： 获取数组该位置的首节点的监视器锁后才能插入从而实现并发安全，利用 CAS 尝试写入，失败则自旋保证成功。
hashcode 计算数值，后取余映射。Key的哈希值与数组长度取模确定该Key在数组中的索引
sys ：采用对象锁。


redis 跳表，保存一个索引。

Java 8在链表长度超过一定阈值（8）时将链表（寻址时间复杂度为O(N)）转换为红黑树（寻址时间复杂度为O(log(N))）

k8s：服务发现、负载均衡，consul，k8s采用全局配置或者内置dns关联。